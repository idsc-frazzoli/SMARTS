name: ppo-baseline

agent:
  state:
    wrapper:
      name: FrameStack
      num_stack: 3
      alpha: 0.570
      degree: 2.0
      asym_cost: True
      com_cost_coef: 0.05
      acc_cost_coef: 7.0
      safety_dist: 20.0
      acc_thres: 3.0
      acc_cost_flatness: 0.020
    features:
      # relative position of goal
      goal_relative_pos: True
      # distance to the center of lane
      distance_to_center: True
      # speed
      speed: True
      # steering
      steering: True
      # a list of heading errors
      heading_errors: [20, continuous]
      # at most eight neighboring vehicles' driving states
      neighbor: 8
      # bird's-eye view gray-scale image with the agent at the center
      # img_gray: (28, 28)

  action:
    type: 1  # 0 for continuous, 1 for discrete

# for agent interface
interface:
  max_episode_steps: 1000
  neighborhood_vehicles:
    radius: 50
  waypoints:
    lookahead: 50  # larger than size of heading errors

policy:
  framework: rllib
  trainer:
    path: ray.rllib.agents.ppo
    name: PPOTrainer

run:
  checkpoint_freq: 5
  checkpoint_at_end: True
  max_failures: 1000
  resume: False
  export_formats: [model, checkpoint]
  stop:
    #episodes_total: 13000
    timesteps_total: 1000000
  config:
    log_level: WARN
    num_workers: 1
    num_gpus: 0
    horizon: 1000
    clip_param: 0.2
    num_sgd_iter: 10
    sgd_minibatch_size: 32
    lr: 1.0e-4
    lr_schedule: [[0, 1.0e-4], [600000, 8.0e-5], [1000000, 1.0e-5]]
    lambda: 0
    
